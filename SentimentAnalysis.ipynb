{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc1b4f8",
   "metadata": {},
   "source": [
    "# Sentiment analysis model using Scikit-learn with IMDb reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983334b",
   "metadata": {},
   "source": [
    "# üìò Introduction\n",
    "\n",
    "**Sentiment analysis** is a natural language processing (NLP) technique used to identify and classify opinions expressed in a piece of text, with the goal of determining whether the sentiment is positive, negative, or neutral. This kind of analysis is widely used in areas such as customer feedback monitoring, social media analysis, and product or service review mining.\n",
    "\n",
    "In this project, we trained a binary sentiment classification model using the well-known **IMDb movie reviews dataset**. Each review is labeled as either positive or negative, making it an ideal dataset for applying and comparing various machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "The main goal of this project was to build a system capable of **automatically predicting whether a movie review is positive or negative**, following these steps:\n",
    "\n",
    "1. Text cleaning and preprocessing  \n",
    "2. Text vectorization using **TF-IDF**  \n",
    "3. Training and comparing multiple classification models  \n",
    "4. Evaluating model performance using key metrics  \n",
    "5. Using the best-performing model to make predictions on new, unseen text\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Models evaluated\n",
    "\n",
    "The following machine learning models were trained and compared:\n",
    "\n",
    "- **Logistic Regression**\n",
    "- **Random Forest**\n",
    "- **Multinomial Naive Bayes**\n",
    "- **Linear Support Vector Classifier (Linear SVC)**\n",
    "- **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "After evaluating precision, recall, F1-score, and training time, the model that showed the **best overall performance was `Logistic Regression`**, offering both high accuracy and fast execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a8ad1",
   "metadata": {},
   "source": [
    "## ‚¨ÜÔ∏è Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41e101",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for etiqueta in ['pos', 'neg']:\n",
    "        folder = os.path.join(path, etiqueta)\n",
    "        print(f\"Loading {etiqueta} from {folder}\")\n",
    "        for file in os.listdir(folder):\n",
    "            with open(os.path.join(folder, file), 'r', encoding='utf-8') as f:\n",
    "                data.append(f.read())\n",
    "                labels.append(1 if etiqueta == 'pos' else 0)\n",
    "    print(f\"Total loaded of {path}: {len(data)} reviews\")\n",
    "    return pd.DataFrame({'text': data, 'label': labels})\n",
    "\n",
    "train = load_data('aclImdb/train')\n",
    "test = load_data('aclImdb/test')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4205b",
   "metadata": {},
   "source": [
    "## üîÉ Text preprocessing\n",
    "\n",
    "```python\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "    return text\n",
    "\n",
    "def apply_preprocessing(df):\n",
    "    df['text'] = df['text'].apply(clear_text)\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285de77",
   "metadata": {},
   "source": [
    "## üî¢ Vectorization (TF-IDF)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_text(train, test):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train = vectorizer.fit_transform(train['text'])\n",
    "    X_test = vectorizer.transform(test['text'])\n",
    "    y_train = train['label']\n",
    "    y_test = test['label']\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb32a0",
   "metadata": {},
   "source": [
    "## üî£ Models training\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "def train_and_eval(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(solver='saga', max_iter=1000, n_jobs=-1),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "        \"Multinomial NB\": MultinomialNB(),\n",
    "        \"Linear SVC\": LinearSVC(max_iter=1000),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(n_jobs=10)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining: {name}\")\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        end = time.time()\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        results[name] = {\n",
    "            \"model\": model,\n",
    "            \"accuracy\": acc,\n",
    "            \"time\": end - start\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def search_better_parameters(X_train, y_train):\n",
    "    model = LogisticRegression(solver='saga', max_iter=1000)\n",
    "    parameters = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l2', 'l1']\n",
    "    }\n",
    "    grid = GridSearchCV(model, parameters, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Better combination:\", grid.best_params_)\n",
    "    print(\"Better accuracy:\", grid.best_score_)\n",
    "\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def save_model(model, vectorizer, path_model='model.joblib', path_vectorizer='vectorizer.joblib'):\n",
    "    joblib.dump(model, path_model)\n",
    "    joblib.dump(vectorizer, path_vectorizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e76dbf",
   "metadata": {},
   "source": [
    "## üì∂ Comparison of results\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_results(results):\n",
    "    names = list(results.keys())\n",
    "    accuracies = [r[\"accuracy\"] for r in results.values()]\n",
    "    times = [r[\"time\"] for r in results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(names, accuracies)\n",
    "    plt.title(\"Accuracy by model\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(names, times)\n",
    "    plt.title(\"Training time by model\")\n",
    "    plt.xlabel(\"Seconds\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd47b98",
   "metadata": {},
   "source": [
    "## üì∂ Run models\n",
    "\n",
    "```python\n",
    "import time\n",
    "from load_dataset import load_data\n",
    "from preprocessing import apply_preprocessing\n",
    "from vectorization import vectorize_text\n",
    "from model import train_and_eval, save_model\n",
    "from print_results import print_results\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# 1. Load dataset\n",
    "train = load_data('aclImdb/train')\n",
    "test = load_data('aclImdb/test')\n",
    "\n",
    "# 2. Clear text\n",
    "train = apply_preprocessing(train)\n",
    "test = apply_preprocessing(test)\n",
    "\n",
    "# 3. Vectorize text\n",
    "X_train, X_test, y_train, y_test, vectorizer = vectorize_text(train, test)\n",
    "\n",
    "# 4.Train and evaluate models\n",
    "results = train_and_eval(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Choose the best model based on accuracy\n",
    "better_name = max(results, key=lambda k: results[k][\"accuracy\"])\n",
    "better_model = results[better_name][\"model\"]\n",
    "\n",
    "print(f\"\\nüß† Better model: {better_name} ({results[better_name]['accuracy']:.4f})\")\n",
    "# Save the best model and vectorizer\n",
    "save_model(better_model, vectorizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training and evaluate completed in {end - start:.2f} seconds\")\n",
    "\n",
    "print_results(results)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df8b6",
   "metadata": {},
   "source": [
    "## üÜí Test model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "from preprocessing import clear_text\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "model = joblib.load('model.joblib')\n",
    "vectorizer = joblib.load('vectorizer.joblib')\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    clean_text = clear_text(text)\n",
    "    vector = vectorizer.transform([clean_text])\n",
    "    pred = model.predict(vector)[0]\n",
    "    return \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    text_usuario = input(\"Enter a review: \")\n",
    "    resultado = predict_sentiment(text_usuario)\n",
    "    print(\"Prediction:\", resultado)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a90a2",
   "metadata": {},
   "source": [
    "# üß† Conclusions\n",
    "\n",
    "This project demonstrated the full process of building a sentiment analysis pipeline using a real-world dataset of movie reviews from IMDb. Through text preprocessing, TF-IDF vectorization, model training, and evaluation, we were able to identify a model capable of accurately predicting whether a review expresses a positive or negative sentiment.\n",
    "\n",
    "Here are the key takeaways:\n",
    "\n",
    "- **Text preprocessing and vectorization are critical** steps in NLP tasks. Cleaning the text and using TF-IDF allowed us to effectively convert raw text into meaningful numerical features.\n",
    "- Among the models evaluated ‚Äî including Logistic Regression, Random Forest, Multinomial Naive Bayes, Linear SVC, and K-Nearest Neighbors ‚Äî **Logistic Regression consistently outperformed the others** in terms of accuracy and training time.\n",
    "- The final model achieved an accuracy of approximately **87%**, demonstrating strong performance on unseen reviews.\n",
    "- Simpler linear models can often outperform more complex models when the data is well-preprocessed, especially in high-dimensional spaces like TF-IDF.\n",
    "- This project can be easily extended to other domains (e.g., product reviews, tweets) or improved with more advanced techniques such as word embeddings, deep learning (LSTM, BERT), or hyperparameter optimization.\n",
    "\n",
    "Overall, this analysis highlights the importance of combining effective preprocessing with model selection, and it provides a strong foundation for more advanced NLP projects.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
